{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3r2JLfz04Hsc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/ml_ad_ai_course/blob/main/3-Attention%20and%20Transformers/18_Transformers_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "\n",
        "© Data Trainers LLC. GPL v 3.0.\n",
        "\n",
        "Author: Axel Sirota\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Inspired highly on the tutorial [NMT with Transformers](https://www.tensorflow.org/text/tutorials/transformer) which takes the code from the original Transformer model paper originally proposed in [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017)."
      ],
      "metadata": {
        "id": "gzf37X_XizQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep"
      ],
      "metadata": {
        "id": "ovyaAhLpa55P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk 'gensim==4.2.0' 'keras-nlp' 'keras-preprocessing' 'tensorflow-text==2.15'"
      ],
      "metadata": {
        "id": "C9BTEOu0PerV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a105afe3-5683-4eda-fc53-bbb700961542"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting gensim==4.2.0\n",
            "  Downloading gensim-4.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-nlp\n",
            "  Downloading keras_nlp-0.14.0-py3-none-any.whl (571 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.8/571.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text==2.15\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (7.0.4)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15) (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (24.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (13.7.1)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.2.0) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (4.12.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text==2.15) (2.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-nlp) (2.31.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (2.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (0.43.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (3.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (2024.6.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15) (3.2.2)\n",
            "Installing collected packages: keras-preprocessing, gensim, tensorflow-text, keras-nlp\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.2\n",
            "    Uninstalling gensim-4.3.2:\n",
            "      Successfully uninstalled gensim-4.3.2\n",
            "Successfully installed gensim-4.2.0 keras-nlp-0.14.0 keras-preprocessing-1.1.2 tensorflow-text-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSaLeBcpqiT5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0fpgYwAtNO2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a83eab6-7974-4c35-e6b8-3663c82b5dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import preprocessing\n",
        "from textblob import TextBlob, Word\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import Model, Input\n",
        "import tensorflow_text as tf_text\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "import time\n",
        "\n",
        "TRACE = False\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer Layers\n",
        "\n",
        "In this demo we will create, from scratch, with the same tools the original Authors had, the Transformer architecture. Why? To understand how it works, why it works, and exactly what is novel!\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The original Transformer diagram</th>\n",
        "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each of the components in these two diagrams will be explained as you progress through the demo.\n"
      ],
      "metadata": {
        "id": "ekM1n0-JdYPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did we have before?"
      ],
      "metadata": {
        "id": "OFbNft2shTPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before, we used Cross Attention or self attention, remember? And for sequence data we basically used it like this:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Seq2Seq with attention</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.dropbox.com/s/r6u7ll5nlt96t9f/seq2seq.png?raw=1\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2FoDkeGhWJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where we input attention with the hidden state to create another updated hidden state we could input into the next cell. And this worked well on medium sized sentences, but was hard to train and unstable. Now that we know this, the Transformer basicaly tried to get rid of the RNN by using **only** attention"
      ],
      "metadata": {
        "id": "b3huYRrUjCAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The embedding and positional encoding layer\n",
        "\n",
        "The inputs to both the encoder and decoder use the same embedding and positional encoding logic.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "vwmp1jC0dxVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This comes straight from the paper\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "UkI09F6zdXnb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ],
      "metadata": {
        "id": "gLC8RPIkd--M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = PositionalEmbedding(5000, 100)"
      ],
      "metadata": {
        "id": "GAnc5AKSeOvn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tf.constant(np.random.randint(1, 5000, size=(5, 26)))\n",
        "response = pos(input)\n",
        "response.shape"
      ],
      "metadata": {
        "id": "zlIZmHLMemkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bfd8e3-a15b-4215-a37a-f3acd61db259"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response._keras_mask"
      ],
      "metadata": {
        "id": "XnIEkVWqg59O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebec019d-9ac3-44f2-ebc1-80d5a58f1f31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 26), dtype=bool, numpy=\n",
              "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True]])>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the shapes look ok?"
      ],
      "metadata": {
        "id": "rfPJ0PvIjXbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add and normalize\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>Add and normalize</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "yvlTAaqqhTBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Use `Add` layer instead of + to propagate masks\n",
        "\n",
        "We will create a BaseAttention layer that inherits the Add+Norm and then each subclass of attention will implement the correct one"
      ],
      "metadata": {
        "id": "P-ZTlyygheuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "OMMyVtc6hLaM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention layer\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "L5Toc-Uukh15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    # We need to compare everything with everything, therefore Q, K and V must be the input\n",
        "    attn_output = self.mha(query=x, value=x, key=x)\n",
        "    # Call self.mha with the correct Q, K and V!\n",
        "    x = self.add([x, attn_output])  # Add the skip connection\n",
        "    x = self.layernorm(x)  # Apply layer normalization\n",
        "    return x"
      ],
      "metadata": {
        "id": "RP4L1Mn-iFCl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it!"
      ],
      "metadata": {
        "id": "316Iwonjm6JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = tf.constant(np.random.randint(1, vocab_size, size=(5, 26)))\n",
        "\n",
        "# First we apply the PositionalEmbedding to embed into what the attention layer expects\n",
        "pos = PositionalEmbedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Then we do the self attention, the n_heads is arbitrary\n",
        "gsa = GlobalSelfAttention(num_heads=3, key_dim=embedding_dim)\n",
        "\n",
        "# Test it\n",
        "response = gsa(pos(input))\n",
        "response.shape"
      ],
      "metadata": {
        "id": "goX_DO0Sk8UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d1a372-a22e-4a9f-9578-0c7a1d1a969b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the shape is the same, since MHA concats all 3 heads and the we add everything"
      ],
      "metadata": {
        "id": "n5lGk8ZMle7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The cross attention layer\n",
        "\n",
        "This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block in the previous demo (and we will copy it).\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The cross attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "J76TA2ZMlrke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=context,\n",
        "        key=context,\n",
        "        return_attention_scores=True\n",
        "    )\n",
        "\n",
        "    # Call self.mha with the correct Q, K and V! We want to return the att scores\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])  # Add the skip connection\n",
        "    x = self.layernorm(x)  # Apply layer normalization\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "OxZMLVBRlMPz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 512\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(5,26)))\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_en, size=(5,30)))\n",
        "\n",
        "\n",
        "pos_es = PositionalEmbedding(vocab_size_es, embedding_dim_es)\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "\n",
        "gsa = GlobalSelfAttention(num_heads=3, key_dim=embedding_dim_es)\n",
        "cross = CrossAttention(num_heads=3, key_dim=embedding_dim_en)\n",
        "\n",
        "\n",
        "context = gsa(pos_es(input_es)) # Forget about the feed forwards to test\n",
        "\n",
        "response = cross(pos_en(input_en), context=context) # Forget about masked attention for now, assume it is the identity\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "R9v9MPbvmVJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76edd74-b266-405e-e4ab-65322a068e54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 30, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the shape is (batch_size, words in sentence in output, embedding_dim) , regardless the input sentence had more words or other embedding dim. We are doing a good move forward!"
      ],
      "metadata": {
        "id": "e5bXoAwvoD8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The causal self attention layer (Masked Multi Headed Attention)\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "PuNqXT82oaRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only big difference in the masked multi headedd attention is that we cannot attend to words in the future, so we will use a mask such that the `Nth` word can only see the first `N-1` words and not all the sentence."
      ],
      "metadata": {
        "id": "_-jDIHkJowAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)  # This is the key!\n",
        "    x = self.add([x, attn_output])  # Add the skip connection\n",
        "    x = self.layernorm(x)  # Apply layer normalization\n",
        "    return x"
      ],
      "metadata": {
        "id": "z6N9U3qvn9Ui"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "PgCBG1b-osuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the diagram above how the query can onlly attend the values for the past"
      ],
      "metadata": {
        "id": "AdE_KY2TpEPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_en = 512\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_en, size=(5,30)))\n",
        "\n",
        "\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "csa = CausalSelfAttention(num_heads =3, key_dim=embedding_dim_en)\n",
        "\n",
        "response = csa(pos_en(input_en))\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "RaHIa0k4oqD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f67d160-4b3a-4ae9-d769-ef5d32fb0e23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 30, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The feed forward network\n",
        "\n",
        "The transformer also includes this point-wise feed-forward network in both the encoder and decoder:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The feed forward network</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "c8_jJCg8pE6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])  # Add the skip connection from the seq\n",
        "    x = self.layer_norm(x)  # Apply layer normalization\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Wu5xFoRjpBty"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The encoder layer\n",
        "\n",
        "The encoder contains a stack of `N` encoder layers. Where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "1Pm7uGPMpqwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model=d_model, dff=dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "3SJl4aJ_pSR0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = tf.constant(np.random.randint(1,vocab_size, size=(5,26)))\n",
        "pos = PositionalEmbedding(vocab_size, embedding_dim)\n",
        "sample_encoder_layer = EncoderLayer(d_model=embedding_dim, num_heads=3, dff=1012)\n",
        "response = sample_encoder_layer(pos(input))\n",
        "response.shape"
      ],
      "metadata": {
        "id": "4NUNWld5uOBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c599ee3-3a64-4520-830e-c838e9fc54cb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The encoder\n",
        "\n",
        "Notice we need to be able to repeat the past EncoderLayer Nx times, so we need another Layer that is able to do exactly that\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "-MzC1z-vuuxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ],
      "metadata": {
        "id": "0TlZP_vVulm0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size = 5000\n",
        "input = tf.constant(np.random.randint(1,vocab_size, size=(3,26)))\n",
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=embedding_dim,\n",
        "                         num_heads=3,\n",
        "                         dff=512,\n",
        "                         vocab_size=vocab_size)\n",
        "response = sample_encoder(input)\n",
        "response.shape"
      ],
      "metadata": {
        "id": "Yf9wGOSMwaEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f64397-eec0-4161-df36-cf6f077e48cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 26, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got our Encoder!! Yahoo!!"
      ],
      "metadata": {
        "id": "uUU9CG_MwtJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The decoder layer\n",
        "\n",
        "Same as before we need a Decoder layer that uses the Attention layers and then another layer to permit having Nx layers of decoding\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The decoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "UR4MuroWw4p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model=d_model, dff=dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ],
      "metadata": {
        "id": "_v5voBr2wywC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 512\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(5,26)))\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(5,30)))\n",
        "\n",
        "pos_en = PositionalEmbedding(vocab_size_en, embedding_dim_en)\n",
        "\n",
        "\n",
        "encoder =  Encoder(num_layers=2, d_model=embedding_dim_es, num_heads=3, dff=512, vocab_size=vocab_size_es)\n",
        "\n",
        "context = encoder(input_es)\n",
        "\n",
        "decoder_layer = DecoderLayer(d_model=embedding_dim_en, num_heads=3, dff=218, dropout_rate=0.2)\n",
        "\n",
        "response = decoder_layer(pos_en(input_en), context=context)\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "aP3yjQeuy2n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935edb3e-189f-4af9-f610-d0cd52f9d95c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 30, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Decoder\n",
        "\n",
        "Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`s:\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Decoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "pog5S-gQz9nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ],
      "metadata": {
        "id": "dEZ2-NRbz1jz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_es = 100\n",
        "vocab_size_es = 5000\n",
        "\n",
        "embedding_dim_en = 512\n",
        "vocab_size_en = 6000\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(5,26)))\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_es, size=(5,30)))\n",
        "\n",
        "encoder =  Encoder(num_layers=2, d_model=embedding_dim_es, num_heads=3, dff=512, vocab_size=vocab_size_es)\n",
        "\n",
        "context = encoder(input_es)\n",
        "\n",
        "decoder = Decoder(num_layers=3, d_model=embedding_dim_en, num_heads=5, dff=124, vocab_size=vocab_size_en)\n",
        "\n",
        "response = decoder(input_en, context=context)\n",
        "\n",
        "response.shape"
      ],
      "metadata": {
        "id": "Mp4zkTw22-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b382eb0-16ec-4806-b6f3-3c54ffec9faa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 30, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer Model\n",
        "\n",
        "You now have `Encoder` and `Decoder`. To complete the `Transformer` model, you need to put them together and add a final linear (`Dense`) layer which converts the resulting vector at each location into output token probabilities.\n",
        "\n",
        "The output of the decoder is the input to this final linear layer.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The transformer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "3r2JLfz04Hsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dff=dff,\n",
        "                          vocab_size=input_vocab_size,\n",
        "                          dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dff=dff,\n",
        "                          vocab_size=target_vocab_size,\n",
        "                          dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ],
      "metadata": {
        "id": "VrXSLwrF36u_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "vocab_size_es = 5000\n",
        "vocab_size_en = 6000\n",
        "\n",
        "num_layers = 4\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# We are supposing the model will translate Spanish to English, so context for CrossAttention will be the spanish input.\n",
        "\n",
        "input_es = tf.constant(np.random.randint(1,vocab_size_es, size=(5,26)))\n",
        "input_en = tf.constant(np.random.randint(1,vocab_size_en, size=(5,30)))\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=embedding_dim,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=vocab_size_es,\n",
        "    target_vocab_size=vocab_size_en,\n",
        "    dropout_rate=dropout_rate)\n",
        "\n",
        "response = transformer((input_es, input_en))\n",
        "response.shape"
      ],
      "metadata": {
        "id": "7CIxbfVx9IBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8325d67-4b2e-40b0-e685-af08c9351f90"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 30, 6000])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()\n"
      ],
      "metadata": {
        "id": "Gy5B7d_E-NDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890d1eba-5aef-42bf-c43b-b3ab38b4c770"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_3 (Encoder)         multiple                  2203648   \n",
            "                                                                 \n",
            " decoder_1 (Decoder)         multiple                  3594448   \n",
            "                                                                 \n",
            " dense_44 (Dense)            multiple                  606000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6404096 (24.43 MB)\n",
            "Trainable params: 6404096 (24.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework: Try to make this transformer a translator and fit it in the spa.txt dataset from the NMT with Attention Lab!"
      ],
      "metadata": {
        "id": "sVIOfqf4kuig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[layer for layer in transformer._flatten_layers()]"
      ],
      "metadata": {
        "id": "VqnWfvyM7XDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f220e8bc-b945-47b5-a293-7deb6ad23140"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Transformer at 0x7c596ce2baf0>,\n",
              " <__main__.Encoder at 0x7c596cdce3e0>,\n",
              " <__main__.PositionalEmbedding at 0x7c596cccc5e0>,\n",
              " <keras.src.layers.core.embedding.Embedding at 0x7c596cccdf30>,\n",
              " <__main__.EncoderLayer at 0x7c596cccd7b0>,\n",
              " <__main__.GlobalSelfAttention at 0x7c596ccce650>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cccead0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cd99390>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cd997b0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cd99bd0>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cd99f90>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd9a170>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cd9a3b0>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cccda80>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cccebf0>,\n",
              " <__main__.FeedForward at 0x7c596ccce500>,\n",
              " <keras.src.engine.sequential.Sequential at 0x7c596cccd990>,\n",
              " <keras.src.engine.input_layer.InputLayer at 0x7c596cd9bb20>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cccdb10>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596ce96d10>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cccf820>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cccca90>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596ccce9e0>,\n",
              " <__main__.EncoderLayer at 0x7c596cccdb70>,\n",
              " <__main__.GlobalSelfAttention at 0x7c596ccce860>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cfd6f50>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cdba230>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cdba650>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cdbaa70>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cdb9a80>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cdb8730>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cdbab30>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cccec20>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd0c6a0>,\n",
              " <__main__.FeedForward at 0x7c596cccfdf0>,\n",
              " <keras.src.engine.sequential.Sequential at 0x7c596cd0d480>,\n",
              " <keras.src.engine.input_layer.InputLayer at 0x7c596cbd42e0>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd0cac0>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd0cbe0>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd0cf70>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd0d7e0>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd0db40>,\n",
              " <__main__.EncoderLayer at 0x7c596cd0df30>,\n",
              " <__main__.GlobalSelfAttention at 0x7c596cd0e140>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd0e350>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbd60e0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbd6680>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbd6aa0>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cbd6e60>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cbd7040>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbd7280>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd0e710>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd0ead0>,\n",
              " <__main__.FeedForward at 0x7c596cd0ece0>,\n",
              " <keras.src.engine.sequential.Sequential at 0x7c596cd0f8b0>,\n",
              " <keras.src.engine.input_layer.InputLayer at 0x7c596cbf0a60>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd0eef0>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd0f010>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd0f3a0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd0fc10>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd0ff70>,\n",
              " <__main__.EncoderLayer at 0x7c596cd1a5f0>,\n",
              " <__main__.GlobalSelfAttention at 0x7c596cd1a800>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd1aa10>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbf2830>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbf2e00>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbf3220>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cbf35e0>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cbf37c0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbf3a00>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd1a0e0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd19d20>,\n",
              " <__main__.FeedForward at 0x7c596cd19b10>,\n",
              " <keras.src.engine.sequential.Sequential at 0x7c596cd18f40>,\n",
              " <keras.src.engine.input_layer.InputLayer at 0x7c596cbf2710>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd19900>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd197e0>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd19450>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd18be0>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd18880>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cccded0>,\n",
              " <__main__.Decoder at 0x7c596cdcded0>,\n",
              " <__main__.PositionalEmbedding at 0x7c596cd1ae90>,\n",
              " <keras.src.layers.core.embedding.Embedding at 0x7c596cd1b0d0>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd1b4f0>,\n",
              " <__main__.DecoderLayer at 0x7c596cd1b850>,\n",
              " <__main__.CausalSelfAttention at 0x7c596cd1ba60>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd1bc70>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc1db40>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc1e020>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc1e440>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cc1e800>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cc1e9e0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc1ec20>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd1bc10>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd38430>,\n",
              " <__main__.CrossAttention at 0x7c596cd386a0>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd388b0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc1fc10>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc1ffd0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc48460>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cc48820>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cc48a00>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc48c40>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd38c70>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd39030>,\n",
              " <__main__.FeedForward at 0x7c596cd1ba00>,\n",
              " <keras.src.engine.sequential.Sequential at 0x7c596cd39e10>,\n",
              " <keras.src.engine.input_layer.InputLayer at 0x7c596cd38c10>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd39450>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd39570>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd39900>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd3a170>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd3a4d0>,\n",
              " <__main__.DecoderLayer at 0x7c596cd3a8c0>,\n",
              " <__main__.CausalSelfAttention at 0x7c596cd3aad0>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd3ace0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cbd4a30>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cdb81c0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc4a230>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cc4a920>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cc4b250>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc4b520>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd3b0a0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd3b460>,\n",
              " <__main__.CrossAttention at 0x7c596cd3b6d0>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd3b8e0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc64250>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc64550>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc64970>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cc64d30>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cc64f10>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc65150>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd3bca0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd3bdc0>,\n",
              " <__main__.FeedForward at 0x7c596cd3aa70>,\n",
              " <keras.src.engine.sequential.Sequential at 0x7c596cd54e80>,\n",
              " <keras.src.engine.input_layer.InputLayer at 0x7c596cc4bfd0>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd544c0>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd545e0>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd54970>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd551e0>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd55540>,\n",
              " <__main__.DecoderLayer at 0x7c596cd55930>,\n",
              " <__main__.CausalSelfAttention at 0x7c596cd55b40>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd55d50>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc67520>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc67a90>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc67eb0>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cc67fa0>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cc84490>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc846d0>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd56110>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd564d0>,\n",
              " <__main__.CrossAttention at 0x7c596cd56740>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd56950>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc85330>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc85ab0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc85ed0>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cc86290>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cc86470>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cc866b0>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd56d10>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd570d0>,\n",
              " <__main__.FeedForward at 0x7c596cd572e0>,\n",
              " <keras.src.engine.sequential.Sequential at 0x7c596cd57eb0>,\n",
              " <keras.src.engine.input_layer.InputLayer at 0x7c596cc867d0>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd574f0>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd57610>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd579a0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd57ee0>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd685b0>,\n",
              " <__main__.DecoderLayer at 0x7c596cd689a0>,\n",
              " <__main__.CausalSelfAttention at 0x7c596cd68bb0>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd68dc0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cca0820>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cca1060>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cca1480>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cca1840>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cca1a20>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cca1c60>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd69180>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd69540>,\n",
              " <__main__.CrossAttention at 0x7c596cd697b0>,\n",
              " <keras.src.layers.attention.multi_head_attention.MultiHeadAttention at 0x7c596cd699c0>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cca1d80>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cca3040>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cca3460>,\n",
              " <keras.src.layers.activation.softmax.Softmax at 0x7c596cca3820>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cca3a00>,\n",
              " <keras.src.layers.core.einsum_dense.EinsumDense at 0x7c596cca3c40>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd69d80>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd6a140>,\n",
              " <__main__.FeedForward at 0x7c596cd6a350>,\n",
              " <keras.src.engine.sequential.Sequential at 0x7c596cd6af20>,\n",
              " <keras.src.engine.input_layer.InputLayer at 0x7c596cca2110>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd6a560>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd6a680>,\n",
              " <keras.src.layers.regularization.dropout.Dropout at 0x7c596cd6aa10>,\n",
              " <keras.src.layers.merging.add.Add at 0x7c596cd6b280>,\n",
              " <keras.src.layers.normalization.layer_normalization.LayerNormalization at 0x7c596cd6b5e0>,\n",
              " <keras.src.layers.core.dense.Dense at 0x7c596cd1ac50>]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8BXvQVm7uFFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}