{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10yt9CFlUu-i"
      },
      "source": [
        "# Generative AI Prompt Engineering\n",
        "\n",
        "In this lab we will use a famous Encoder-Decoder LLM: Flan-T5. You will first do simple tasks to get your hands dirty.\n",
        "\n",
        "Then you will learn about few shot prompting, and see how at a certain point the LLM just cannot do the task.\n",
        "\n",
        "You will finish by testing the different possible configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZCLA3RFUu-l"
      },
      "source": [
        "## Install Required Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "r4pq7IowUu-n"
      },
      "source": [
        "Now install the required packages to use Hugging Face transformers and datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1d-Q-oDUu-o",
        "outputId": "3246b843-1b87-464d-9cea-9c5c15e1bf36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers==4.35.2 datasets==2.15.0  --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "DVVL_S2pUu-q"
      },
      "source": [
        "Load the datasets, Large Language Model (LLM), tokenizer, and configurator. Do not worry if you do not understand yet all of those components - they will be described and discussed later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "id": "mYiuh1IJUu-q",
        "outputId": "46a2e6ae-8150-439d-fb08-1af88c311db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doing Simple Tasks with Flan-T5"
      ],
      "metadata": {
        "id": "dopRAyYmVgIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we wil do simple sentiment analysis so you get the gist of how to use these LLMs. You will use the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index)"
      ],
      "metadata": {
        "id": "NnQoZRA5VkXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1rqdJDfWC34",
        "outputId": "ef7da594-5307-4e07-fe22-4cb2f3af6d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "huggingface_dataset_name = \"imdb\"\n",
        "\n",
        "dataset = load_dataset(\"imdb\") # Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MYNJ9JPWU6X",
        "outputId": "81a2b0a5-94df-49e2-c40f-d8b9622ca368"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use from the train set, but it is the same for us now"
      ],
      "metadata": {
        "id": "_vtWCMBZWW1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_random_review_and_label():\n",
        "  random_index = np.random.randint(1, 25000)\n",
        "  random_review = dataset['train'][random_index]['text']\n",
        "  label = dataset['train'][random_index]['label']\n",
        "  return random_review, label\n",
        "\n",
        "random_review, label = get_random_review_and_label()\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "print(f'Review: \\n\\n{random_review}')\n",
        "print(dash_line)\n",
        "print(f'Label: {label}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTgAzyGUWWWl",
        "outputId": "8c66524a-7898-4042-dc9d-1bc6d4941e31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: \n",
            "\n",
            "It could have been a better film. It does drag at points, and the central story shifts from Boyer completing his mission to Boyer avenging Wanda Hendrix's death, but Graham Greene is an author who is really hard to spoil. His stories are all morality tales, due to his own considerations of Catholicism, guilt and innocence (very relative terms in his world view), and the human condition.<br /><br />Boyer is Luis Denard, a well-known concert pianist, who has sided with the Republicans in the Spanish Civil War. He has been sent to England to try to carry through an arms purchase deal that is desperately needed. Unfortunately for Denard he is literally on his own - everyone of his contacts turns out to be a willing turncoat for the Falagists of Spain. In particular Katina Paxinou (Mrs. Melendez) a grim boarding house keeper, and Peter Lorre (Mr. Contreras) a teacher of an \"esperanto\" type international language. Wanda Hendrix is the drudge of a girl (Else) who works for Mrs. Melendez. The local diplomat, Licata (Victor Francken) is already a willing associate of the Falangists.<br /><br />The Brits (Holmes Herbert, Miles Mander, and best - if not worst - of the lot, George Coulouris) don't give much hope to Boyer's cause (which he soon grasps may be Britain's before long). Herbert and Mander just retreat behind the official policy of neutrality ordered by the Ramsay MacDonald's and Stanley Baldwin's governments during the Civil War. Coulouris here is a typical Col. Blimp type - always impeccable in his native English diction, he is sharp in showing his dislike for foreigners in general.<br /><br />The one ray of hope is Lauren Bacall (Rose Cullen), here trying to play her role as well as she can - but she can't really. She's an aristocrat - the daughter of a Press lord. It was Bacall's second film, and (sad to say) almost sank her long career. She does act well, but the spark she showed in her first film was due to the dual effect of starring with Humphrey Bogart and being directed by Howard Hawks. Boyer is a fine actor, but he's not Bogie, and Herman Shumlin is not Hawks. Her next film returned her to Bogie and Hawks again, and her star resumed it's ascendancy.<br /><br />It's a bleak film (as was the novel). Boyer's mission never succeeds, as he has too many hidden foes all over the place. But the villains are likewise also losers - frequently with their lives.<br /><br />With Dan Seymour as a suspicious foreign tenant of Katina Paxinou (and the man who destroys her). It is well worth watching to catch the Warner's lot of character actors doing their best given the weakness in direction.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use the model! For that we need to use the Tokenizer to transform the text into the \"model language\" (more on this during the course). Also we need to download the model. Remember we are going to use a `TFAutoModelForSeq2SeqLM` model"
      ],
      "metadata": {
        "id": "ymB1rlYNXKDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-base'\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUkkP28YXJqO",
        "outputId": "1e64c2e7-d0e7-4c38-9542-d05c59ad6527"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = random_review[:50]\n",
        "print(f'Review trimmed: {sentence}')\n",
        "\n",
        "sentence_encoded = tokenizer(sentence)\n",
        "\n",
        "sentence_decoded = tokenizer.decode(\n",
        "        sentence_encoded[\"input_ids\"],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print('ENCODED SENTENCE:')\n",
        "print(sentence_encoded[\"input_ids\"])\n",
        "print('\\nDECODED SENTENCE:')\n",
        "print(sentence_decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmXUvVjGX4nV",
        "outputId": "c119a3ac-48d7-41ea-979a-edee050dfa55"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review trimmed: This was the second of the series of 6 \"classic Ta\n",
            "ENCODED SENTENCE:\n",
            "[100, 47, 8, 511, 13, 8, 939, 13, 431, 96, 4057, 447, 2067, 1]\n",
            "\n",
            "DECODED SENTENCE:\n",
            "This was the second of the series of 6 \"classic Ta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's call the model. As this is a TFAutoModelForSeq2SeqLM this means that is a LLM for seq2seq tasks, like summarizing or text generation, so let's put our prompt that way."
      ],
      "metadata": {
        "id": "7FuJakPgZKAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "review, label = get_random_review_and_label()\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Analyze the sentiment of the following review:\n",
        "\n",
        "{review}\n",
        "\n",
        "Sentiment:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "input = tokenizer(prompt)"
      ],
      "metadata": {
        "id": "4f9uN1GqZHL4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(tf.constant([input['input_ids']]), max_new_tokens=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5zbZd9jacww",
        "outputId": "87efbbed-4b92-4ded-beb5-aa3f7d2d08ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[   0, 1465,    1]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(\n",
        "        model.generate(tf.constant([input['input_ids']]), max_new_tokens=50)[0],\n",
        "        skip_special_tokens=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xC1NmLJtc9m8",
        "outputId": "ceca0aa9-73bd-4a18-e7d8-d97e38af4201"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And what was the real sentiment? Remember in this dataset `0` is negative and `1` is positive"
      ],
      "metadata": {
        "id": "cq3GKMWixf17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLf8XDHSxdMM",
        "outputId": "296ccc13-3bef-476f-b76c-f224c770ac8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rZa63vfUu-r"
      },
      "source": [
        "## Summarize News without Prompt Engineering\n",
        "\n",
        "In this use case, you will be generating a summary of news with Flan-T5.\n",
        "\n",
        "Let's upload some simple dialogues from the dialogsum Hugging Face dataset. This dataset contains 10,000+ articles with the corresponding manually labeled summaries."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHa-vFSNabP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [],
        "id": "h5-aI0MGUu-r"
      },
      "outputs": [],
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)  # Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset\n",
        "print(dataset.keys)\n",
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOGobYwPWSUO",
        "outputId": "1c01702f-f570-4c55-e336-333b07de66fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method keys of DatasetDict object at 0x78649045fc90>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (12460, 4), 'validation': (500, 4), 'test': (1500, 4)}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "qp6hynk2Uu-s"
      },
      "source": [
        "Print a couple of dialogues with their baseline summaries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_dialogue_and_summary():\n",
        "  # Implement this method\n",
        "\n",
        "  random_index = np.random.randint(1, 10000)\n",
        "  random_dialogue = dataset['train'][random_index]['dialogue']\n",
        "  summary = dataset['train'][random_index]['summary']\n",
        "  return random_dialogue, summary\n",
        "\n",
        "random_dialogue, summary = get_random_dialogue_and_summary()\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "print(f'Dialogue: \\n\\n{random_dialogue}')\n",
        "print(dash_line)\n",
        "print(f'Summary: {summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lgoc3px0nIU",
        "outputId": "8a677414-d001-456e-a3e8-596c20b94264"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue: \n",
            "\n",
            "#Person1#: I'd like a cup of coffee and a cheeseburger, please.\n",
            "#Person2#: I'm sorry, but we don't have any burgers at the moment.\n",
            "#Person1#: But you always serve your whole menu for breakfast, lunch and dinner. That's why I come here.\n",
            "#Person2#: You're right, but one of our cooks is sick. So we had to take some things off the menu for a while. If you want to come back in half an hour, we'll definitely have our normal lunch menu.\n",
            "#Person1#: That's OK, I'm really hungry. Let me see. I'll still take the coffee and I'll have a bacon and egg sandwich, instead, please.\n",
            "#Person2#: Do you want breakfast potatoes with that?\n",
            "#Person1#: No, thank you.\n",
            "#Person2#: OK, your total is $6.50.\n",
            "#Person1#: Here is a 10.\n",
            "#Person2#: And here's your change and receipt.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary: #Person1# wants a cheeseburger and a coffee, but the burger isn't available at the moment, so #Person1# takes an egg sandwich instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "-ve7EcUaUu-w"
      },
      "source": [
        "Test the tokenizer encoding and decoding a simple sentence:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrUuKzRVUu-w"
      },
      "source": [
        "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. **Prompt engineering** is an act of a human changing the **prompt** (input) to improve the response for a given task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0OvBzzdUu-w",
        "outputId": "a05b403d-b7ca-48c3-edb2-33126867d3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1188 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Hi, Tim. So, are you doing some last-minute shopping before the weekend?\n",
            "#Person2#: Well, actually, I'm looking for supplies to put together 72-hour kits for each member of my family.\n",
            "#Person1#: [A] 72-hour kit? What's that?\n",
            "#Person2#: Basically, a 72-hour kit contains emergency supplies you would need to sustain yourself for three days in case of an emergency, like an earthquake.\n",
            "#Person1#: An earthquake?! We haven't had an earthquake in years.\n",
            "#Person2#: Well, you never know; you have to be prepared. Hey, if earthquakes don't get you, it could be a flood, hurricane, snowstorm, power outage, fire, alien attack. [Alien attack!] Well, you never know. Think of any situation in which you might find yourself without the basic necessities of life, including shelter, food, and water, for over a period of time.\n",
            "#Person1#: Hum. So, what do you keep in a 42-hour, um, ... I mean 72-kit?\n",
            "#Person2#: Well, you should have enough food and water to last you three days, and you might want to pack a basic water filter or water purification tablets in case your only water source turns out to be a murky pool of bug-infested water. [Ugh!] Hey, sometimes you don't have a choice, and as for food, you should keep it simple: food that requires no preparation and that doesn't spoil. And no canned goods because they are often too heavy and bulky. [Okay, that makes sense.] And unless you have a can opener or the can has a pull-tab lid, you'll have to use a rock or something to open them. [Ah, instant mashed green beens.] Yeah, and oh, energy bars, beef jerky, and a mix of nuts, raisins, and chocolate are possibilities.\n",
            "#Person1#: Huh, the food might be nasty, but I guess you could survive ... barely.\n",
            "#Person2#: Well, the food doesn't have to taste bad; just select things that are easy to prepare, and you might want to include some basic comfort foods like a couple of candy bars. Then, you have to decide on the type of shelter you might need.\n",
            "#Person1#: A hotel sounds nice.\n",
            "#Person2#: Yeah, but that's really not an option. The reality is that you might have to evacuate to a shelter, possibly with hundreds or thousands of other people.\n",
            "#Person1#: That doesn't sound very fun ... everyone packed together like sardines in a can. Unsanitary conditions. Disease.\n",
            "#Person2#: Ah, now you're sounding paranoid, but if a shelter isn't available, you might be completely on your own, so I always pack an emergency sleeping bag or small, lightweight tent in the event that I have to survive on the street or in a park.\n",
            "#Person1#: Wow.\n",
            "#Person2#: And among other things, you should pack a flashlight, portable radio, extra batteries, a small first-aid kit, personal items like a toothbrush or toothpaste ... Having a change of clothing is also important.\n",
            "#Person1#: What about money? I have a credit card.\n",
            "#Person2#: Right. Like that's going to help when the power is out. You'd better be prepared with coins and cash, and having small bills is a must.\n",
            "#Person1#: So, what do you do to communicate with other family members in case you get separated?\n",
            "#Person2#: Oh, in that case? I always pack two-way radios to communicate with the group. You can never depend on cell phones. [Okay.] Plus, you should decide on a meeting point in case your family gets separated.\n",
            "#Person1#: Well, that sounds like a detailed plan, definitely.\n",
            "#Person2#: Oh, that's not all. You never know what weather conditions you might encounter, so packing a rain poncho, a jacket, and something to start a fire with could be very useful.\n",
            "#Person1#: Like Matches?\n",
            "#Person2#: Matches? If You drop those in a puddle of water, you're toast. You need to pack at least three forms of fire starter: a magnifying glass, a high-quality lighter, and waterproof matches.\n",
            "#Person1#: Wow. I never thought about those either. So, what do you do if you have small kids? They'd probably go stir-crazy under such conditions.\n",
            "#Person2#: You're exactly right, so a little extra preparation for them is needed. If you have to evacuate to a shelter to wait out a disaster, kids soon will be bored out of their minds, so you have to pack small card games, paper, or something like pencils or crayons to draw with.\n",
            "#Person1#: You know, preparing a 72-hour kit makes perfect sense ...\n",
            "#Person2#: Yeah, but most people thinking about it after it is too late.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "Tim tells #Person1# that he is putting together 72-hour kits for his family in case of emergencies. They discuss what to prepare for a 72-hour kit, like food, water, shelter, fire starters, things for little kids, etc. #Person1# thinks a 72-hour kit makes perfect sense but #Person1# seems to believe it unnecessary, but Tim insists on its importance.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Without prompt engineering:\n",
            "#Person1#: [A] 72-hour kit? #Person2#: Well, you never know. Hey, if earthquakes don't get you, it could be a flood, hurricane, snowstorm,\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Hi, Gary. How handsome you are in the blue coat!\n",
            "#Person2#: Thank you. I'm very glad to hear that.\n",
            "#Person1#: I like your new coat very much. Where did you get it?\n",
            "#Person2#: Well, I got it from the department store where there are many clothes, from shirt to skirt, from jacket to coat.\n",
            "#Person1#: Oh. I know that store. Clothes there are expensive for me. How much did it cost you?\n",
            "#Person2#: Not as much as the saleswoman asked for. She wanted $90.\n",
            "#Person1#: So. how much did you pay for it in the end?\n",
            "#Person2#: I tried to make her bring down to half the price.\n",
            "#Person1#: Wow! How smart you are! In fact, I have been attracted by a cool hat in that store for a long time. Can you help me bring its price down?\n",
            "#Person2#: I'm not sure, but I can have a try.\n",
            "#Person1#: Then, thank you very much.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "Gary tells #Person1# he managed to bring down the price of his new blue coat to half. #Person1# requests Gary to help #Person1# bargain for a cool hat.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Without prompt engineering:\n",
            "#Person1#: Hi, Gary. How handsome you are in the blue coat! #Person2#: Well, I got it from the department store where there are many clothes, from shirt to skirt, from jacket to coat.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  3\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Finding an excuse is obviously down your alley. And trying to avoid taking care of the children is what you are good at.\n",
            "#Person2#: Listen, Nancy. If I don't work hard, I will be laid off.\n",
            "#Person1#: You are passing the buck. I'll be hanged if I ask you to go to the park with us.\n",
            "#Person2#: Come on, Nancy. Play it cool. I go with you and burn the middle night oil tonight.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "Nancy is angry because #Person2# doesn't take care of the kids, so #Person2# agrees to help and work at night.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Without prompt engineering:\n",
            "Person1#: I'm not a good employee. I'm a good worker.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    dialogue, summary = get_random_dialogue_and_summary()\n",
        "    inputs = tokenizer(dialogue)\n",
        "    model_output =  model.generate(tf.constant([inputs['input_ids']]), max_new_tokens=50)[0] # Generate model output\n",
        "    output = tokenizer.decode(\n",
        "        model_output,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'Dialogue:\\n{dialogue}')\n",
        "    print(dash_line)\n",
        "    print(f'Summary:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'Model Summary - Without prompt engineering:\\n{output}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvSILLIHUu-x"
      },
      "source": [
        "You can see that the guesses of the model make some sense, but it doesn't seem to be sure what task it is supposed to accomplish. Seems it just makes up the next sentence in the dialogue. Prompt engineering can help here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dVGYvChUu-y"
      },
      "source": [
        "## Summarize Dialogue with an Instruction Prompt\n",
        "\n",
        "Prompt engineering is an important concept in using foundation models for text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSapC4PiUu-y"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 - Zero Shot Inference with an Instruction Prompt\n",
        "\n",
        "In order to instruct the model to perform a task - summarize a dialogue - you can take the dialogue and convert it into an instruction prompt. This is often called **zero shot inference**.  \n",
        "Wrap the dialogue in a descriptive instruction and see how the generated text will change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9eLywFzUu-y",
        "outputId": "754f1210-425f-4d63-f0e9-70f718076159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Hello. Can I help you?\n",
            "#Person2#: Hello. Is my laundry ready? My room number is 210.\n",
            "#Person1#: I'm afraid it is still being washed.\n",
            "#Person2#: Can you take the stain off?\n",
            "#Person1#: Yes, we can. But you need wait a moment.\n",
            "#Person2#: That's right. Can I get it back in the afternoon? I really need them tonight.\n",
            "#Person1#: Yes, it will be ready then.\n",
            "#Person2#: OK. By the way, please get them pressed.\n",
            "#Person1#: No problem.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "#Person2# requests #Person1# to get the laundry ready and pressed by the afternoon.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Zero shot inference prompt engineering:\n",
            "#Person1#: Hello. Is your laundry ready?\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Hello!\n",
            "#Person2#: Hello! May I speak to Mr. White?\n",
            "#Person1#: Speaking.\n",
            "#Person2#: This is Michael's mother. I want to ask for two days' leave for him.\n",
            "#Person1#: Oh, what's the matter with him?\n",
            "#Person2#: He has a bad cold. I want to take him to see a doctor.\n",
            "#Person1#: I'm sorry to hear that. I hope he will get well soon.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "Michael's mother calls Mr. White to ask for two days' leave for Michael because Michael has a bad cold.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Zero shot inference prompt engineering:\n",
            "#Person1#: Hello!\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  3\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "#Person1#: Ikebana, it's an art of flowers and it's quite different from Western style flower arrangement because in Ikebana's theory you can decorate one flower, only with one flower. \n",
            "#Person2#: What's another Japanese art? \n",
            "#Person1#: Japanese art? Mm, Ikebana! Tea Ceremony! \n",
            "#Person2#: Tea Ceremony! Tell me about Tea Ceremony. \n",
            "#Person1#: Tea Ceremony! It's, there's a certain way you have to make tea, not only make tea, there's a certain way to for example wipe a bowl. \n",
            "#Person2#: I see. \n",
            "#Person1#: Yes, with one piece of cloth, and you need to learn how to fold the cloth so that you you use each part of the cloth only once to wipe the bowl \n",
            "#Person2#: How do you learn that? Is that something that your mother would teach you? How do people learn that? \n",
            "#Person1#: There's some professionals for both flower arrnagement and tea ceremony. So you need to go to a school, yes, and learn from your teacher and their heirarchy and the organization and you need to pass each test to go, climb up the ladder in the heirarchy. \n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "#Person1# tells #Person2# about Ikebana, the Japanese art of flowers, and the Tea Ceremony in Japan. There're some professionals for both flower arrangement and tea ceremony.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Zero shot inference prompt engineering:\n",
            "Ikebana, Ikebana, Ikebana, Ikebana, Ikebana, Ikebana, Ikebana, Ikebana, Ikebana, Ikebana,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    dialogue, summary = get_random_dialogue_and_summary()\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt)\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(tf.constant([inputs['input_ids']]), max_new_tokens=50)[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'Dialogue:\\n{dialogue}')\n",
        "    print(dash_line)\n",
        "    print(f'Summary:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'Model Summary - Zero shot inference prompt engineering:\\n{output}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyK6NBA2Uu-z"
      },
      "source": [
        "This is much better! But the model still does not pick up on the nuance of the conversations though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4X_Lb25Uu-1"
      },
      "source": [
        "## Summarize Dialogue with One Shot and Few Shot Inference\n",
        "\n",
        "**One shot and few shot inference** are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9k8MEJnlUu-1"
      },
      "source": [
        "## One Shot Inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": [],
        "id": "ZjO6HueVUu-1"
      },
      "outputs": [],
      "source": [
        "def make_prompt_and_return_real_summary(number_of_shots):\n",
        "    prompt = ''\n",
        "    for i in range(number_of_shots):\n",
        "        dialogue, summary = get_random_dialogue_and_summary()\n",
        "\n",
        "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
        "        prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\n",
        "{summary}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    dialogue_to_analise , real_summary = get_random_dialogue_and_summary()\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue_to_analise}\n",
        "\n",
        "Summary:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    return prompt, real_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "31MWrQgEUu-2"
      },
      "source": [
        "Construct the prompt to perform one shot inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0kDBdJyUu-2",
        "outputId": "a2f836d3-6e93-44ba-a53c-b14dbff83e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Have you ever been to Xi ' an?\n",
            "#Person2#: Yes, I ' Ve been there several times on business trips. But I have never really seen the terra-cotta warriors as it is outside the city.\n",
            "#Person1#: I ' Ve heard many people saying that it is a place worth touring. I really want to see the old walls and terra-cotta warriors one day. Of course I won ' t miss the local food either. You know, the sites interests a food in scenery, food is a key factor when visiting a place.\n",
            "#Person2#: I agree. As long as the food is not too bizarre once I saw some people eating insects. That is frightening.\n",
            "#Person1#: Sure it is. Is it convenient to get there by plane?\n",
            "#Person2#: Well, the airport is quite far from the downtown area, but it is still more convenient than taking the train.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1# and #Person2# are talking about Xi'an. #Person1# wants to see the site-interests and try the local food. #Person2# tells #Person1# it's more convenient to go to Xi'an by air.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Can I help you?\n",
            "#Person2#: Yes, I'd like a hamburger, please.\n",
            "#Person1#: Do you want it with everything?\n",
            "#Person2#: No. Don't put mustard in it.\n",
            "#Person1#: All right. You have a hamburger without mustard. Anything else?\n",
            "#Person2#: Sprite with ice, please.\n",
            "#Person1#: OK. Here you are.\n",
            "#Person2#: Thank you.\n",
            "\n",
            "Summary:\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "one_shot_prompt, real_summary = make_prompt_and_return_real_summary(1)\n",
        "\n",
        "print(one_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "HnBpMxxIUu-2"
      },
      "source": [
        "Now pass this prompt to perform the one shot inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxQ6LML9Uu-2",
        "outputId": "a697bd56-31b4-4e46-9909-2cf6e7467f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Good morning. Can I help you?\n",
            "#Person2#: Yes. I wonder if you have a one-bedroom apartment to rent.\n",
            "#Person1#: Let me check. Yes, we have one. It's on Nanjing Street, near a shopping center and a subway station.\n",
            "#Person2#: Sounds nice. Does it face south?\n",
            "#Person1#: Well. the bedroom faces east and the living room north. But it looks out on a beautiful park.\n",
            "#Person2#: Mmm, is the living room large?\n",
            "#Person1#: Yes. it's quite big. And there's a small kitchen and a bathroom as well. It's very comfortable.\n",
            "#Person2#: Well, what's the rent per month?\n",
            "#Person1#: 800 yuan.\n",
            "#Person2#: Mmm. it's more than I have in mind. Let me think it over. I'll call you back in a day or two.\n",
            "#Person1#: Certainly.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person2# wants to rent an apartment. #Person1# recommends one and introduces its location, orientations, rooms, and rent. #Person2# thinks it's expensive and will think it over.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Hello, Mrs. Turnbull. How are you? \n",
            "#Person2#: Fine, thanks. How's your boy, Jack? \n",
            "#Person1#: He's a bit tired. You know, he goes to school at eight o'clock every morning. He doesn't get home till after four. then he does his homework after tea. It often takes him a couple of hours to finish \n",
            "#Person2#: Poor boy. They work hard at school nowadays, don't they? Does he like it? \n",
            "#Person1#: School, you mean? Yes, he does. He likes his teachers, and that always makes adifference. \n",
            "#Person2#: Yes, it does. Does he go to school by bus? \n",
            "#Person1#: No, he walks. He likes walking. He meets some of his friends at the corner andthey go together. \n",
            "#Person2#: What does he do when it rains? \n",
            "#Person1#: My husband takes him in the car. He passes the school on the way to the office. \n",
            "\n",
            "Summary:\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "#Person1# tells Mrs. Turnbull about #Person1#'s boy, Jack, who is a bit tired because of school but likes it. Jack usually walks to school but takes the car when it rains.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - One shot inference prompt engineering:\n",
            "<pad> Jack goes to school at eight o'clock every morning. He doesn't get home until after four. He does his homework after tea. He walks to school. He meets some of his friends at the corner andthey go together. He\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example 2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Hi, what brings you to my office today?\n",
            "#Person2#: I have been getting really short of breath, and my coach wanted to have a doctor check me out.\n",
            "#Person1#: Have you had the flu lately?\n",
            "#Person2#: No, I have been pretty healthy. I just have trouble taking a really deep breath.\n",
            "#Person1#: Have you ever been tested for allergies?\n",
            "#Person2#: Peaches make me break out, but I don't have any other allergies.\n",
            "#Person1#: Does this happen all the time or maybe a little more in the cold weather?\n",
            "#Person2#: I have noticed that it is worse when I am under stress, like during finals week.\n",
            "#Person1#: I feel that you should see a pulmonary specialist to check for asthma.\n",
            "#Person2#: I appreciate the referral, doctor.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person2# has trouble breathing lately and it's worse when under stress. #Person1# suggests seeing a pulmonary specialist to check for asthma.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: The guy on the phone wanted to speak to Miss Hall and it sounded urgent.\n",
            "#Person2#: But she's not coming until after lunch today. Why don't you call her on her cell phone?\n",
            "#Person1#: That's what I was going to do, but the caller hung up and didn't leave his number.\n",
            "#Person2#: Well, that's bad. I guess if it's really important, he'll call again. But you should call Miss Hall anyway, just to let her know.\n",
            "\n",
            "Summary:\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "#Person1# says the caller urgently needs Miss Hall who's not coming but did not leave a number. #Person2# still advises #Person1# to let Miss Hall know.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - One shot inference prompt engineering:\n",
            "<pad> The caller hung up and didn't leave his number.</s>\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example 3\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Which hotel are we going to, Mr. Zhang? \n",
            "#Person2#: Jindu Hotel. Its No. 12 in Jining Rood not far from the downtown. I've booked a double room with a shower for you. \n",
            "#Person1#: Thanks a lot. \n",
            "#Person3#: (They entered the lobby of Jindu Hotel)Good evening, sir. What can I do for you? \n",
            "#Person2#: I made a reservation with you last week. \n",
            "#Person3#: Your name, please? \n",
            "#Person2#: Simon Pemberton from Canada. \n",
            "#Person3#: Oh yes, you did. (To Mr. Pemberton) Welcome to our hotel. Please fill in the form. \n",
            "#Person1#: Okay. (After finishing the form) Here you are, lady. By the way, have you got 24 hour room service? \n",
            "#Person3#: Sure. We also serve both Chinese food and Western food. Here's the key to Room 201. The bellman will show you to your room. Hope you will enjoy your stay here. \n",
            "#Person1#: Thank you very much. See you. \n",
            "\n",
            "Summary:\n",
            "\n",
            "Mr. Zhang tells Simon Pemberton that he has booked a double room for Simon at Jindu Hotel. #Person3# helps Simon check-in at the hotel.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Hi, Cole. What can I do for you?\n",
            "#Person2#: if you have a few minutes, I'd like to talk to you about my future at this company.\n",
            "#Person1#: sure, have a seat.\n",
            "#Person2#: thanks.\n",
            "#Person1#: let me just grab your file. How long have you worked for us now?\n",
            "#Person2#: I've worked here as a sales representative for about a year now.\n",
            "#Person1#: one year already? It's amazing how time flies like that. Are you enjoying your job?\n",
            "#Person2#: yes, but I'd like to have a chance at job advancement.\n",
            "#Person1#: I see. What job did you have in mind.\n",
            "#Person2#: well, I've noticed that is a position available as a sales manager.\n",
            "#Person1#: do you understand what duties that job would entail?\n",
            "#Person2#: yes. I would be directly responsible for all of the sales representatives in my department. I assume there'd be more meetings, paperwork, and other responsibilities, too.\n",
            "#Person1#: that's right. Do you have any experience in management?\n",
            "#Person2#: yes. In fact if you look at my resume, you can see that I was a manager before I started this job.\n",
            "#Person1#: well, I think you'd be the perfect candidate for the position. According to company policy, you'll still have to go through the formal application procedures though, so fill this application form in\n",
            "#Person2#: ok. Thanks for your support.\n",
            "\n",
            "Summary:\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "Cole's worked as a sales representative for about a year and would like a promotion to sales manager. #Person1# thinks he'd be the perfect candidate for the position and tells him to go through the formal application procedures.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - One shot inference prompt engineering:\n",
            "<pad> Cole is looking for a new job. He's been working for the company for about a year now. He's looking for a promotion.</s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range (3):\n",
        "  one_shot_prompt, real_summary = make_prompt_and_return_real_summary(1)\n",
        "  inputs = tokenizer(one_shot_prompt)\n",
        "  output = tokenizer.decode(model.generate(tf.constant([inputs['input_ids']]), max_new_tokens=50)[0]) # Get output\n",
        "  print(dash_line)\n",
        "  print(f'Example {i + 1}')\n",
        "  print(dash_line)\n",
        "  print(f'Dialogue:\\n{one_shot_prompt}')\n",
        "  print(dash_line)\n",
        "  print(f'Summary:\\n{real_summary}')\n",
        "  print(dash_line)\n",
        "  print(f'Model Summary - One shot inference prompt engineering:\\n{output}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "lQnxULZdUu-3"
      },
      "source": [
        "### Few Shot Inference\n",
        "\n",
        "Let's explore few shot inference by adding two more full dialogue-summary pairs to your prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzj7eM2CUu-4",
        "outputId": "001c8942-8eac-484a-e2b1-a26fcbafab66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Are you ready to go shopping?\n",
            "#Person2#: Not yet. I'm not finished with my research yet.\n",
            "#Person1#: What research?\n",
            "#Person2#: Reading my fashion magazines! How do you think I know so much about all the latest trends?\n",
            "#Person1#: But they're just ads. . .\n",
            "#Person2#: Duh. . . That's the point. The people in the ads are wearing what's in. Plus, there are articles on new trends. . .\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1# wants to go shopping with #Person2# but #Person2# hasn't finished reading fashion magazines.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Mom, I'm starving.\n",
            "#Person2#: Here are some biscuits. Why are you back so early today?\n",
            "#Person1#: My teacher had a sudden stomachache, so the class was cut shot. You?\n",
            "#Person2#: Me what?\n",
            "#Person1#: You are cooking at least two hours earlier than the usual.\n",
            "#Person2#: It's not for us.\n",
            "#Person1#: Then it's for Dad, isn't it? It's so unfair!\n",
            "#Person2#: Don't be a smarty-pants. It's for Grandma Wang.\n",
            "#Person1#: What was that again?\n",
            "#Person2#: It's for Grandma Wang. She is sick and her only daughter went abroad weeks ago. So she needs our help.\n",
            "#Person1#: I'm sorry, I didn't know that. But I wanna help.\n",
            "#Person2#: Umm, let me think for a moment. We can meet her together after I finish cooking.\n",
            "#Person1#: I'll get knee to knee with her.\n",
            "#Person2#: Good boy. I can only imagine how happy she will be to see you.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1#'s back early because #Person1#'s teacher had a sudden stomachache. Mum's cooking for Grandma Wang because she's sick and her only daughter went abroad. #Person1# offers to help.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: The dinner was really good. It knocked my socks off.\n",
            "#Person2#: That's very kind of you to say so. Let's try some after-dinner wines.\n",
            "#Person1#: Great. Sweet wines are my favorite. They always make a great finish to a decisions meal.\n",
            "#Person2#: Do you prefer brandy or ports.\n",
            "#Person1#: Port, please.\n",
            "#Person2#: Excellent choice. I love its smooth flavor.\n",
            "#Person1#: The port is quite exquisite. It must have spent years aging in barrels. Am I right?\n",
            "#Person2#: Yes. You always have a good nose for wines.\n",
            "#Person1#: Next time we are about to dinner we should try some Canadian ice wine.\n",
            "#Person2#: Oh, what's that?\n",
            "#Person1#: It's made from naturally frozen grapes.\n",
            "#Person2#: Why not? It sounds great.\n",
            "#Person1#: OK, here's to your health.\n",
            "#Person2#: Thanks. Cheers.\n",
            "#Person1#: Cheers.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person2# suggests trying some after-dinner wines and #Person1# chooses port. #Person1# loves its taste and suggests trying some Canadian ice wine next time.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: We're now staying at the Holiday Inn. How do we get to the airport?\n",
            "#Person2#: We have shuttle buses leaving for the airport from downtown every 20 minutes The bus stops at Holiday Inn as well. Be sure to get on the bus before 30.\n",
            "#Person1#: What is the earliest one?\n",
            "#Person2#: At 7 am. It takes about one hour to get to the airport. So it'll be right for your timing.\n",
            "#Person1#: OK, thank you. By the way, what can I do if I want to put off my flight?\n",
            "#Person2#: Your tickets are valid for one year. If you want to change flight, you just go to the airline's ticketing office and have it reconfirmed. But you have to fly the same airlines.\n",
            "#Person1#: OK. What can I do if l want to cancel the ticket?\n",
            "#Person2#: In that case. you can get a refund with a certain percentage of the original fare deducted as the service charge.\n",
            "#Person1#: That's reasonable. OK, thank you very much.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1# asks #Person2# about the information on transportation to the airport and how to change and cancel the flight.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Good afternoon, would you mind if I sat here?\n",
            "#Person2#: Of course not.\n",
            "#Person1#: I'm Jack. What's your name?\n",
            "#Person2#: Laura.\n",
            "#Person1#: Do you like this place?\n",
            "#Person2#: I don't think it's very nice. And my father doesn't like it. But my mother likes it very much. So we often come here.\n",
            "#Person1#: How often?\n",
            "#Person2#: Well, we come here almost every month.\n",
            "#Person1#: Who's that?\n",
            "#Person2#: It's my mother. She's fond of swimming. And the man beside her is my father.\n",
            "#Person1#: Do you like swimming?\n",
            "#Person2#: No, I hate swimming. I prefer playing tennis.\n",
            "\n",
            "Summary:\n",
            "\n",
            "Laura and Jack sit together and talk. Laura tells Jack her family comes to the place almost every month.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What's the area of your country?\n",
            "#Person2#: It's not very big. It's a little over half a million square kilometers.\n",
            "#Person1#: That sounds quite big! How many people live there?\n",
            "#Person2#: There are about 30 million people in my country. Most of them live in the north.\n",
            "#Person1#: What's the average income?\n",
            "#Person2#: That's the really hard to say. I think most people earn about two thousand dollars a month, if you convert the money from our currency into dollars.\n",
            "#Person1#: So your country is fairly rich.\n",
            "#Person2#: I think we are richer than most countries, but not as rich as countries in western Europe. Our biggest problem at the moment is unemployment, which is roughly 8 %. It has doubled over the last four years.\n",
            "#Person1#: Unemployment in my country is a fraction of that.\n",
            "\n",
            "Summary:\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "#Person2# talks with #Person1# about #Person2#'s country which has a big area, a high population and a relatively high average income. The unemployment in #Person2#'s country is serious while in #Person1#'s country not.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Few shot inference prompt engineering:\n",
            "The area of the country is about half a million square kilometers.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example 2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: So, Misaki, you're from Japan, right?\n",
            "#Person2#: Yes, I'm from Akita, the northern Japan.\n",
            "#Person1#: What is it like?\n",
            "#Person2#: There are a lot of mountains. And you can actually see colors changing on them in the fall.\n",
            "#Person1#: Sounds beautiful.\n",
            "#Person2#: Yeah, in my home, Yashima town, there are only 6,000 people. But there is an amazing waterfall which I consider to be the best in Japan, really tall and wide.\n",
            "#Person1#: Cool. So when is the last time you went home?\n",
            "#Person2#: 7 days ago. My parents still live there.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1# asks about where Misaki is from and what is it like. Misaki speaks highly of her hometown.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Daniel, what are you doing here? Aren't you supposed to be at school now?\n",
            "#Person2#: The same question to you.\n",
            "#Person1#: Well, we shall make it a secret between us.\n",
            "#Person2#: Deal, Where is Gucci?\n",
            "#Person1#: She is the cheer-leader. They are required to put on a performance. Look! Here they come.\n",
            "#Person2#: Oh, look at her. She looks like one is the basketball babies in NBA.\n",
            "#Person1#: I want to be like her.\n",
            "#Person2#: Get real. Don't be so pathetic.\n",
            "#Person1#: Hey, young man! I'm your sis. Don't talk to me like that.\n",
            "#Person2#: Oh, man, look at her, go! She is amazing! She should be my sis.\n",
            "#Person1#: Boy, you've got such a crush on her, haven't you?\n",
            "#Person2#: Yes, I want to be her boyfriend. Just like in the movie.\n",
            "#Person1#: Are you out of your mind?\n",
            "#Person2#: She told me she liked my new haircut. She thought it was cool.\n",
            "#Person1#: Go ahead, have a try and be a joke.\n",
            "\n",
            "Summary:\n",
            "\n",
            "Daniel and #Person2# skip the school. Daniel finds that #Person2# has got a crush on Gucci, but Daniel thinks #Person2# is not able to make Gucci his girlfriend.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: welcome back! How was your vacation? \n",
            "#Person2#: it was fantastic, but I'm glad to be back! Being a tourist is really tiring! \n",
            "#Person1#: where did you end up going? \n",
            "#Person2#: because it's off-season, we got a really good package deal to Paris, so we went there. \n",
            "#Person1#: I've always wanted to go to Paris. The Eiffel Tower is one of the most famous tourist attractions in the world! Did you go to the top? \n",
            "#Person2#: that was the first thing we did. I have a few pictures. Do you want to see them?\n",
            "#Person1#: sure. What's this one a picture of? \n",
            "#Person2#: oh, that's a picture of me on our fourth day of travelling. I'm standing next to a famous founation in the centre of the city. \n",
            "#Person1#: you don't look very happy in that picture. \n",
            "#Person2#: no , by that time, I was sick of sightseeing. I had had enough of art galleries, cathedrals, fountains, statues, and palaces! \n",
            "#Person1#: so what did you do? \n",
            "#Person2#: we spent that afternoon walking around a flea market. We had a few coffees, watched a movie, and went for a swim in the pool at the hotel. \n",
            "#Person1#: my travel agent always reminds me to plan a day of relaxing for every 3 days of sightseeing. Did you go to the Louvre?\n",
            "#Person2#: of course! You can't go to Paris without going to their famous art gallery! I was surprised by how small the Mona Lisa was though. \n",
            "#Person1#: that's what everyone says! I can't wait to see it for myself some day. \n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person2# describes the trip to Paris to #Person1# and shares the pictures with #Person1#. #Person1# gets excited about going to Paris too.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Do you know who's just got married? Old Mc Donald.\n",
            "#Person2#: Never! He's over 80, isn't he?\n",
            "#Person1#: He's nearly 90.\n",
            "#Person2#: Good gracious! Are you sure?\n",
            "#Person1#: I am. Whatsmore, his wife is 84.\n",
            "#Person2#: Is she really?\n",
            "#Person1#: Yes, she is. And guess what, he is her sixth husband.\n",
            "#Person2#: Really? Quite a woman, isn't she?\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1# tells #Person2# old Mc Donald has just married an 84-year-old woman.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What do you like to do with your free time?\n",
            "#Person2#: Study English.\n",
            "#Person1#: You mean you like to study English? Why?\n",
            "#Person2#: It gives me great satisfaction.\n",
            "#Person1#: Studying English wouldn't give me any satisfaction. It's hard work.\n",
            "#Person2#: I don't mind the work. I think it's worthwhile.\n",
            "\n",
            "Summary:\n",
            "\n",
            "Studying English is a great satisfaction to #Person2# but hard work to #Person1#.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: I'm here with Margaret Seabrook, the CEO of creative toys. In today's show, we're going to discuss the hottest new toy of two thousand seventeen, the Super Spinner. Margaret, welcome.\n",
            "#Person2#: Thank you, Brian. It's great to be here.\n",
            "#Person1#: OK, so tell us about this new toy.\n",
            "#Person2#: Well. It's similar to a relaxation ball in its function, but it's useful for anyone who has problems focusing.\n",
            "#Person1#: So how does it work?\n",
            "#Person2#: It's about the size of a cookie and it has 3 small round parts that can move in any direction. Basically, you just hold it in between your thumb and middle finger and spin it. That's it.\n",
            "#Person1#: That's it?\n",
            "#Person2#: Yeah, it's very popular. Not only with children, but with adults as well.\n",
            "#Person1#: A professor at MIT by the name of Jill Mean Lee has publicly stated there is no scientific or medical evidence for your claims about its benefits. Many schools also have banned the toy, saying it leads to a lack of focus in the classroom.\n",
            "#Person2#: Well, that professor is allowed to have her opinion.\n",
            "#Person1#: Fair enough, and who invented it?\n",
            "#Person2#: Catherine Hettinger, a chemical engineer, was first believed to be its creator. But then we found that an IT professional named Scott McCoskry was the actual inventor.\n",
            "#Person1#: It's time for a commercial break. More with Margaret Seabrook in a moment.\n",
            "\n",
            "Summary:\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "Margaret Seabrook is telling the audience about the new toy, the Super Spinner. It is similar to a relaxation ball in its function, but it's useful for anyone who has problems focusing. It's popular with children as well as adults.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Few shot inference prompt engineering:\n",
            "#Person1#: I'm here with Margaret Seabrook, CEO of creative toys. In today's show, we're going to discuss the hottest new toy of two thousand seventeen, the Super Spinner.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example 3\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Dialogue:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What does Human Resources Department do?\n",
            "#Person2#: Hiring, firing, training, insurances, benefits, retirement plans, salary, vacation.\n",
            "#Person1#: They take care of a lot of things.\n",
            "#Person2#: But most of time, they provide assistances.\n",
            "#Person1#: What do you mean?\n",
            "#Person2#: Say if the Engineering Department wants to hire a person, they will request HR to find candidates.\n",
            "#Person1#: Yes?\n",
            "#Person2#: The Engineering Manager and his team will interview the candidates. HR will also be involved with the interview, but basically arranging the schedule and explaining the benefits.\n",
            "#Person1#: OK.\n",
            "#Person2#: Then the Engineering Manager will choose the candidate.\n",
            "#Person1#: I see.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person2# answers #Person1# the functions of the Human Resource Department and #Person2# tells #Person1# examples.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Wow! What a beautiful scenic spot! It's so open. And just breathe that fresh air, you can almost taste its freshness.\n",
            "#Person2#: You can have a bird view of Guiling City from the top of the mountain.\n",
            "#Person1#: Wonderful! I'll often come here for mountain climbing.\n",
            "#Person2#: You should. Many Guiliners, especially the old and the young, will climb mountains here in the morning.\n",
            "#Person1#: No wonder people say\n",
            "#Person2#: Quick! Pass me your binoculars. Look at that bird... I've never seen one of those before. It's indigenous to Guiling, and an endangered species too. This is lucky!\n",
            "#Person1#: I didn't know you liked bird-watching.\n",
            "#Person2#: I don't really. I just like wildlife, and you don't get to see too much of it in the city. This place is full of it.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1# and #Person2# are enjoying a bird view of the Guiling City from the top of the mountains and are watching birds.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Linda, John and I are going to get married next week. \n",
            "#Person2#: That's wonderful. Congratulations. \n",
            "#Person1#: Thank you, Linda. We would love you to come to our wedding. \n",
            "#Person2#: I'd love to. What date is it? \n",
            "#Person1#: It's May 1st. \n",
            "#Person2#: What day is that? \n",
            "#Person1#: It's Saturday. Could you make the time? \n",
            "#Person2#: Yes, sure. What time will the wedding begin? \n",
            "#Person1#: At nine sharp. \n",
            "#Person2#: Very good! I'm looking forward to it. Please give my best regards to John. And I wish you best luck! \n",
            "#Person1#: Thanks. \n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1# invites Linda to #Person1#'s wedding. Linda accepts the invitation and congratulates them.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: I am certain I am going to fail my English test.\n",
            "#Person2#: Why are you so pessimistic?\n",
            "#Person1#: Because it's impossible to improve listening level within a week.\n",
            "#Person2#: Only for this reason?\n",
            "#Person1#: Yes, I think my vocabulary and reading comprehension are not very bad.\n",
            "#Person2#: Don't worry about it. Practice makes perfect. I will lend you some listening materials, so that you can practise more before the exam. I believe you can pass the exam with good preparation.\n",
            "#Person1#: Oh, thank you. That's very nice of you.\n",
            "\n",
            "Summary:\n",
            "\n",
            "#Person1# thinks #Person1#'ll fail the English test because #Person1# can't improve listening fast. #Person2# will lend #Person1# materials.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Hello, Ingrid. What are you trying to do here? All these advertisements in papers. My goodness, you're not looking very happy, are you?\n",
            "#Person2#: My parents want me to study in England. They threw all these to me and asked me to find out about schools in England.\n",
            "#Person1#: Hang on, I'll switch on your reading lamp. It's so dark here.\n",
            "#Person2#: I can hardly understand these advertisements, and I don't know what to do.\n",
            "#Person1#: Do you really want to study in England?\n",
            "#Person2#: Well, my parents want me to. How can I know which school is better?\n",
            "#Person1#: The kettle is boiling. Why don't we make some coffee? And then we can look at the papers together. OK, just a minute. Would you like some sugar?\n",
            "\n",
            "Summary:\n",
            "\n",
            "Ingrid tells #Person1# she is pushed by her parents to find a school in England. #Person1# offers to help.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Oh, I am starving.\n",
            "#Person2#: Me too. Shall we eat out? There is a new French restaurant down the street.\n",
            "#Person1#: Oh, forget about it. I went there with a friend last week. The menu was all in French and I just couldn't read it.\n",
            "#Person2#: That's what you are paying for.\n",
            "#Person1#: Maybe. But I should say everything was expensive and nothing was to my satisfaction.\n",
            "#Person2#: Then how about the Italian restaurant on the next block.\n",
            "#Person1#: Well. I ate out almost every day last week. Let's just eat in today.\n",
            "#Person2#: But I am not in the mood to cook.\n",
            "#Person1#: I will cook then. In fact, I am sick and tired of restaurant. I just want a home-cook meal.\n",
            "\n",
            "Summary:\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary:\n",
            "#Person2# suggests they eat out. But #Person1# wants a home-cook meal, because #Person2# ate out almost every day last week, and promises to cook.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model Summary - Few shot inference prompt engineering:\n",
            "Person1 and Person2 are going to eat out.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range (3):\n",
        "  few_shot_prompt, real_summary = make_prompt_and_return_real_summary(5)\n",
        "  inputs = tokenizer(few_shot_prompt)\n",
        "  output = tokenizer.decode(\n",
        "      model.generate(tf.constant([inputs['input_ids']]), max_new_tokens=50)[0],\n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "\n",
        "  print(dash_line)\n",
        "  print(f'Example {i + 1}')\n",
        "  print(dash_line)\n",
        "  print(f'Dialogue:\\n{few_shot_prompt}')\n",
        "  print(dash_line)\n",
        "  print(f'Summary:\\n{real_summary}')\n",
        "  print(dash_line)\n",
        "  print(f'Model Summary - Few shot inference prompt engineering:\\n{output}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "zc9ivJFoUu-4"
      },
      "source": [
        "In this case, few shot did not provide much of an improvement over one shot inference.  And, anything above 5 or 6 shot will typically not help much, either.  Also, you need to make sure that you do not exceed the model's input-context length which, in our case, if 512 tokens.  Anything above the context length will be ignored.\n",
        "\n",
        "However, you can see that feeding in at least one full example (one shot) provides the model with more information and qualitatively improves the summary overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "F5zdN8qMUu-5"
      },
      "source": [
        "## Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p18wGMrpUu-5",
        "outputId": "decbd2eb-b6e0-45c7-c951-73f5ba341978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - FEW SHOT:\n",
            "People are hungry. #S vecm to meet the other indiana diner who offered them suggestions. One can learn how do cost in restaurant food as they eat out almost every other day of it....\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# suggests they eat out. But #Person1# wants a home-cook meal, because #Person2# ate out almost every day last week, and promises to cook.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "generation_config = GenerationConfig(max_new_tokens=100, do_sample=True, temperature=2.0)\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt)\n",
        "output = tokenizer.decode(\n",
        "    model.generate(tf.constant([inputs['input_ids']]), generation_config=generation_config)[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{real_summary}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWIEyXa9Uu-5"
      },
      "source": [
        "Comments related to the choice of the parameters in the code cell above:\n",
        "- Choosing `max_new_tokens=10` will make the output text too short, so the dialogue summary will be cut.\n",
        "- Putting `do_sample = True` and changing the temperature value you get more flexibility in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZmI_YXHUu-6"
      },
      "source": [
        "As you can see, prompt engineering can take you a long way for this use case, but there are some limitations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chkduQrNUu-6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      }
    ],
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}